# -*- coding: utf-8 -*-
"""Modeling - HOME CREDIT INDONESIA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ybhu_3D8afdTJX10vV1AhngJRiNKEkKg
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install gdown

"""# Import atau Ekstrak Dataset"""

import gdown
import zipfile

# URL file dari Google Drive
url = 'https://drive.google.com/uc?id=1SSX3DcOx91zzRWDwCp9z91s5pJT_aVPC'

# Nama file setelah diunduh
output = '/content/home-credit-default-risk.zip'

# Unduh file dari Google Drive
gdown.download(url, output, quiet=False)

# Ekstrak file zip
with zipfile.ZipFile(output, 'r') as zip_ref:
    zip_ref.extractall('/content/home-credit-default-risk')

"""Mengatur tampilan pandas"""

import pandas as pd

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

"""# Deskripsi Data

1. Data Utama (application_{train|test}.csv):
   - Data utama, terbagi menjadi dua file untuk Data Latih (dengan TARGET) dan Data Uji (tanpa TARGET).

2. Biro Kredit (bureau.csv):
   - Data kredit sebelumnya dari klien yang dilaporkan ke Biro Kredit (untuk klien yang memiliki pinjaman dalam sampel data kami).

3. Saldo Bulanan Biro Kredit (bureau_balance.csv):
   - Saldo bulanan dari kredit sebelumnya di Biro Kredit.

4. Saldo Bulanan Kredit POS dan Tunai (POS_CASH_balance.csv):
   - Snapshot saldo bulanan dari kredit POS (point of sales) dan tunai sebelumnya yang dimiliki pelamar dengan Home Credit.

5. Saldo Bulanan Kartu Kredit (credit_card_balance.csv):
   - Snapshot saldo bulanan dari kartu kredit sebelumnya yang dimiliki pelamar dengan Home Credit.

6. Aplikasi Sebelumnya (previous_application.csv):
   - Semua aplikasi sebelumnya untuk pinjaman Home Credit dari klien yang memiliki pinjaman dalam sampel data kami.

7. Riwayat Pembayaran (installments_payments.csv):
   - Riwayat pembayaran untuk kredit yang telah dibayarkan sebelumnya di Home Credit yang terkait dengan pinjaman dalam sampel kami.

8. Deskripsi Kolom Home Credit (HomeCredit_columns_description.csv):
   - File yang berisi deskripsi untuk kolom-kolom dalam berbagai file data.

"""

path_bureau = '/content/home-credit-default-risk/bureau.csv'
path_bureau_balance = '/content/home-credit-default-risk/bureau_balance.csv'
path_POS_CASH_balance = '/content/home-credit-default-risk/POS_CASH_balance.csv'
path_credit_card_balance = '/content/home-credit-default-risk/credit_card_balance.csv'
path_previous_application = '/content/home-credit-default-risk/previous_application.csv'
path_installments_payments = '/content/home-credit-default-risk/installments_payments.csv'

df_bureau = pd.read_csv(path_bureau)
df_bureau_balance = pd.read_csv(path_bureau_balance)
df_POS_CASH_balance = pd.read_csv(path_POS_CASH_balance)
df_credit_card_balance = pd.read_csv(path_credit_card_balance)
df_previous_application = pd.read_csv(path_previous_application)
df_installments_payments = pd.read_csv(path_installments_payments)

print('Ukuran data POS_CASH_balance:', df_POS_CASH_balance.shape)
print('Ukuran data bureau_balance:', df_bureau_balance.shape)
print('Ukuran data previous_application:', df_previous_application.shape)
print('Ukuran data installments_payments:', df_installments_payments.shape)
print('Ukuran data credit_card_balance:', df_credit_card_balance.shape)
print('Ukuran data bureau:', df_bureau.shape)

"""# CLEANING, PREPROCESSING, FEATURE ENGINEERING
Pada tahap ini, dilakukan penghapusan atau modifikasi beberapa fitur agar dapat disusun dalam format yang cocok untuk proses pemodelan

# df_application_train
"""

df_application_train = pd.read_csv('/content/home-credit-default-risk/application_train.csv')
df_application_train.head()

print('Ukuran data application_train:', df_application_train.shape)

df_application_train.nunique()

"""Menghapus kolom "SK_ID_CURR" karena kolom tersebut hanyalah sebuah identifikasi unik untuk setiap entri dalam dataset"""

df_application_train.drop(columns=['SK_ID_CURR'], inplace=True)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(20,20))
sns.heatmap(df_application_train.corr(numeric_only=True))

"""Di sini, jika terdapat pasangan fitur dengan korelasi yang tinggi, salah satunya akan dipilih. Tidak ada standar pasti untuk menentukan nilai korelasi yang dianggap tinggi, tetapi biasanya angka 0.7 digunakan sebagai acuan."""

import numpy as np

corr_matrix = df_application_train.corr(numeric_only=True).abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop_hicorr = [column for column in upper.columns if any(upper[column] > 0.7)]

to_drop_hicorr

df_application_train.drop(to_drop_hicorr, axis=1, inplace=True)
print("Ukuran DataFrame setelah penghapusan kolom:", df_application_train.shape)

"""# Check Categorical Features"""

df_application_train.select_dtypes(include='object').nunique()

df_application_train['ORGANIZATION_TYPE'].value_counts()

"""Menggabungkan kategori yang serupa menjadi satu kategori yang lebih umum, sehingga memperjelas dan menyederhanakan analisis."""

category_mapping = {
    'Business Entity Type 1': 'Business Entity',  # Menggabungkan tipe bisnis 1
    'Business Entity Type 2': 'Business Entity',  # Menggabungkan tipe bisnis 2
    'Business Entity Type 3': 'Business Entity',  # Menggabungkan tipe bisnis 3
    'Self-employed': 'Self-employed',  # Pekerja mandiri
    'Government': 'Government',  # Pemerintah
    'School': 'Education',  # Sekolah
    'Kindergarten': 'Education',  # Taman kanak-kanak
    'University': 'Education',  # Universitas
    'Medicine': 'Healthcare',  # Kesehatan - Bidang Kedokteran
    'Construction': 'Construction',  # Konstruksi
    'Trade: type 1': 'Trade',  # Tipe perdagangan 1
    'Trade: type 2': 'Trade',  # Tipe perdagangan 2
    'Trade: type 3': 'Trade',  # Tipe perdagangan 3
    'Trade: type 4': 'Trade',  # Tipe perdagangan 4
    'Trade: type 5': 'Trade',  # Tipe perdagangan 5
    'Trade: type 6': 'Trade',  # Tipe perdagangan 6
    'Trade: type 7': 'Trade',  # Tipe perdagangan 7
    'Industry: type 1': 'Industry',  # Tipe industri 1
    'Industry: type 2': 'Industry',  # Tipe industri 2
    'Industry: type 3': 'Industry',  # Tipe industri 3
    'Industry: type 4': 'Industry',  # Tipe industri 4
    'Industry: type 5': 'Industry',  # Tipe industri 5
    'Industry: type 6': 'Industry',  # Tipe industri 6
    'Industry: type 7': 'Industry',  # Tipe industri 7
    'Industry: type 8': 'Industry',  # Tipe industri 8
    'Industry: type 9': 'Industry',  # Tipe industri 9
    'Industry: type 10': 'Industry',  # Tipe industri 10
    'Industry: type 11': 'Industry',  # Tipe industri 11
    'Industry: type 12': 'Industry',  # Tipe industri 12
    'Industry: type 13': 'Industry',  # Tipe industri 13
    'Security': 'Security',  # Keamanan
    'Housing': 'Housing',  # Perumahan
    'Military': 'Military',  # Militer
    'Bank': 'Finance',  # Bank
    'Agriculture': 'Agriculture',  # Pertanian
    'Police': 'Government',  # Kepolisian
    'Transport: type 1': 'Transport',  # Tipe transportasi 1
    'Transport: type 2': 'Transport',  # Tipe transportasi 2
    'Transport: type 3': 'Transport',  # Tipe transportasi 3
    'Transport: type 4': 'Transport',  # Tipe transportasi 4
    'Postal': 'Government',  # Pos
    'Security Ministries': 'Government',  # Kementerian Keamanan
    'Restaurant': 'Hospitality',  # Restoran
    'Services': 'Other Services',  # Layanan Lainnya
    'Hotel': 'Hospitality',  # Hotel
    'Electricity': 'Utilities',  # Listrik
    'Insurance': 'Finance',  # Asuransi
    'Telecom': 'Utilities',  # Telekomunikasi
    'Emergency': 'Government',  # Darurat
    'Advertising': 'Marketing',  # Periklanan
    'Realtor': 'Real Estate',  # Agen Properti
    'Culture': 'Arts and Culture',  # Kebudayaan
    'Legal Services': 'Legal',  # Layanan Hukum
    'Cleaning': 'Services',  # Layanan Pembersihan
    'Mobile': 'Telecom',  # Telepon Seluler
    'Religion': 'Religious',  # Agama
}

# Mengganti nilai kategori menggunakan mapping
df_application_train['ORGANIZATION_TYPE'] = df_application_train['ORGANIZATION_TYPE'].replace(category_mapping)

# Menyesuaikan kategori 'XNA' menjadi 'Other'
df_application_train['ORGANIZATION_TYPE'] = df_application_train['ORGANIZATION_TYPE'].replace('XNA', 'Other')
print(df_application_train['ORGANIZATION_TYPE'].value_counts())

df_application_train['ORGANIZATION_TYPE'].nunique()

for col in df_application_train.select_dtypes(include='object').columns.tolist():
    print(df_application_train[col].value_counts(normalize=True)*100)
    print('\n')

"""Fitur yang sangat didominasi oleh salah satu nilai saja akan dibuang pada tahap ini."""

df_application_train.drop(['NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'NAME_HOUSING_TYPE', 'HOUSETYPE_MODE', 'EMERGENCYSTATE_MODE'], axis=1, inplace=True)
print("Ukuran DataFrame setelah penghapusan kolom:", df_application_train.shape)

"""# MISSING VALUES
Missing Value Checking
"""

total = df_application_train.isnull().sum().sort_values(ascending=False)
percent = (df_application_train.isnull().sum() / len(df_application_train) * 100).sort_values(ascending=False)
missing_application_train_data = pd.concat([total, percent], axis=1, keys=['Total', 'Persentase'])
missing_application_train_data

"""Menghapus kolom yang memiliki persentase nilai yang hilang yang cukup tinggi, yang mungkin sulit untuk diimputasi dengan tepat atau dapat mengganggu kinerja model."""

import pandas as pd

column_drop_miss = ['COMMONAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'FONDKAPREMONT_MODE', 'YEARS_BUILD_AVG',
                     'OWN_CAR_AGE', 'LANDAREA_AVG', 'BASEMENTAREA_AVG', 'EXT_SOURCE_1', 'NONLIVINGAREA_AVG',
                     'WALLSMATERIAL_MODE', 'APARTMENTS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG',
                     'YEARS_BEGINEXPLUATATION_AVG']

df_application_train.drop(columns=column_drop_miss, axis=1, inplace=True)
print("Ukuran DataFrame setelah penghapusan kolom:", df_application_train.shape)

df_application_train.info()

"""Membuat kolom baru yang akan menyimpan rasio antara jumlah kredit yang dimiliki oleh pelamar dengan total pendapatan mereka. Kolom ini membantu kita dalam menganalisis seberapa besar bagian dari pendapatan seseorang yang digunakan untuk membayar kredit."""

df_application_train['CREDIT_INCOME_PERCENT'] = df_application_train['AMT_CREDIT'] / df_application_train['AMT_INCOME_TOTAL']
df_application_train['CREDIT_INCOME_PERCENT'].head()

"""Hapus kolom yang sudah tidak digunakan"""

df_application_train.drop(['AMT_CREDIT', 'AMT_INCOME_TOTAL'], axis=1, inplace=True)

"""Kolom ini akan berisi total jumlah permintaan kredit yang dilakukan oleh pelamar dalam berbagai rentang waktu, seperti per jam, per hari, per minggu, per bulan, per kuartal, dan per tahun. Dengan kata lain, kita menjumlahkan semua nilai permintaan kredit dari kolom yang ada untuk mendapatkan total permintaan kredit pelamar."""

df_application_train['TOTAL_REQ_CREDIT'] = df_application_train[['AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY',
                                                                 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON',
                                                                 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']].sum(axis=1)
df_application_train['TOTAL_REQ_CREDIT'].head()

"""Hapus kolom yang sudah tidak digunakan"""

df_application_train.drop(['AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY',
                           'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON',
                           'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR'], axis=1, inplace=True)

"""Konversi DAYS_BIRTH menjadi usia dalam tahun"""

df_application_train['AGE'] = round(-df_application_train['DAYS_BIRTH'] / 365, 1)

"""Konversi DAYS_EMPLOYED menjadi tahun bekerja"""

df_application_train['YEARS_EMPLOYED'] = round(-df_application_train['DAYS_EMPLOYED'] / 365, 1)

"""Hapus kolom yang sudah tidak digunakan"""

df_application_train.drop(['DAYS_BIRTH', 'DAYS_EMPLOYED'], axis=1, inplace=True)

df_application_train.head()

"""# Exploratory Data Analysis"""

df_application_train.describe()

"""CNT_CHILDREN: Nilai maksimum (19) terlihat agak aneh. Ini bisa menjadi indikasi outlier atau data yang tidak biasa."""

import matplotlib.pyplot as plt

# Plot histogram sebelum penanganan outlier
plt.figure(figsize=(10, 6))
plt.hist(df_application_train['CNT_CHILDREN'], bins=20, color='skyblue', edgecolor='black')
plt.title('Distribusi CNT_CHILDREN')
plt.xlabel('Jumlah Anak')
plt.ylabel('Frekuensi')
plt.grid(True)
plt.show()

import seaborn as sns

# Plot box plot untuk kolom CNT_CHILDREN
plt.figure(figsize=(8, 6))
sns.boxplot(data=df_application_train, x='CNT_CHILDREN', color='skyblue')
plt.title('Box Plot CNT_CHILDREN')
plt.xlabel('Jumlah Anak')
plt.show()

import matplotlib.pyplot as plt

# Filter data untuk jumlah anak lebih dari 10
outlier_data = df_application_train[df_application_train['CNT_CHILDREN'] > 10]

# Plot histogram jumlah anak yang lebih dari 10
plt.figure(figsize=(8, 6))
plt.hist(outlier_data['CNT_CHILDREN'], bins=20, color='skyblue', edgecolor='black')
plt.title('Histogram Jumlah Anak yang Lebih dari 10')
plt.xlabel('Jumlah Anak')
plt.ylabel('Frekuensi')
plt.grid(True)
plt.show()

"""Hapus baris dengan indeks outlier"""

print("Jumlah baris sebelum penghapusan outlier:", len(df_application_train))

df_application_train.drop(df_application_train[df_application_train['CNT_CHILDREN'] > 10].index, inplace=True)

print("Jumlah baris setelah penghapusan outlier:", len(df_application_train))

"""Analisis Variabel Kategorikal periksa frekuensi masing-masing kategori dan hubungannya dengan variabel target."""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# List kolom yang merupakan variabel kategorikal
categorical_cols = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE']

for col in categorical_cols:
    category_counts = df_application_train[col].value_counts()

    # Plot frekuensi kategori
    plt.figure(figsize=(10, 6))
    sns.barplot(x=category_counts.index, y=category_counts.values, hue=None, palette='viridis')
    plt.title(f'Frequency of Categories in {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)
    plt.show()

    # Cek hubungan dengan variabel target
    plt.figure(figsize=(10, 6))
    sns.countplot(data=df_application_train, x=col, hue='TARGET', palette='viridis')
    plt.title(f'Relationship between {col} and TARGET')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)
    plt.legend(title='TARGET', loc='upper right')
    plt.show()

"""Eksplorasi Variabel Target"""

import matplotlib.pyplot as plt

# Menghitung jumlah nilai pada kolom target
target_counts = df_application_train['TARGET'].value_counts()

# Plotting diagram lingkaran
plt.figure(figsize=(8, 6))
plt.pie(target_counts, labels=target_counts.index, colors=['skyblue', 'salmon'], autopct='%1.1f%%', startangle=140)
plt.title('Distribusi Target')
plt.show()

"""# Dataset Splitting"""

from sklearn.model_selection import train_test_split

X = df_application_train.drop(columns=['TARGET'])
y = df_application_train['TARGET']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""# Modeling"""

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier
import numpy as np

# Tentukan kolom numerik dan kategorikal
numerical_cols = ['CNT_CHILDREN', 'REGION_POPULATION_RELATIVE', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH',
                  'FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL',
                  'REGION_RATING_CLIENT', 'HOUR_APPR_PROCESS_START', 'REG_REGION_NOT_LIVE_REGION',
                  'REG_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY',
                  'EXT_SOURCE_2', 'EXT_SOURCE_3', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE',
                  'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4',
                  'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9',
                  'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14',
                  'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19',
                  'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'CREDIT_INCOME_PERCENT', 'TOTAL_REQ_CREDIT', 'AGE',
                  'YEARS_EMPLOYED']

categorical_cols = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',
                    'NAME_FAMILY_STATUS', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE']

"""Impute missing value untuk numerik menggunakan median dan kategorik dengan most_frequent. Serta dilakukan Scalling untuk data numerik menggunakan standardScaler dan dilakukan encoding untuk data kategorik menggunakan OneHotencoder

# Model Logistic Regression
"""

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

logistic_model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(solver='liblinear', max_iter=1000, penalty='l2'))
])

# Menambahkan parameter yang akan diuji
param_space_lr = {
    "classifier__C": np.logspace(-3, 3, 7),
    "classifier__fit_intercept": [True, False]
}

model_lr = RandomizedSearchCV(logistic_model, param_distributions=param_space_lr, n_iter=10, cv=3, random_state=42)

model_lr.fit(X_train, y_train)

# Tampilkan hasil regresi logistik
print("Best Parameters (Logistic Regression):", model_lr.best_params_)
print("Training Accuracy (Logistic Regression):", model_lr.score(X_train, y_train))
print("Model Best Score (Logistic Regression):", model_lr.best_score_)
print("Test Accuracy (Logistic Regression):", model_lr.score(X_test, y_test))

"""# Evaluation Model LR"""

from sklearn.metrics import classification_report, confusion_matrix

# Prediksi label untuk data uji
y_pred = model_lr.predict(X_test)

# Evaluasi performa model
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Menghitung probabilitas prediksi untuk kelas positif
y_pred_proba = model_lr.predict_proba(X_test)[:, 1]

# Menghitung nilai ROC AUC
roc_auc = roc_auc_score(y_test, y_pred_proba)
print("ROC AUC Score:", roc_auc)

# Menghitung kurva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Plotting kurva ROC
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""# Feature Importance Model LR"""

categorical_encoder = model_lr.best_estimator_.named_steps['preprocessor'].transformers_[1][1]
encoded_categories = categorical_encoder.named_steps['onehot'].get_feature_names_out(categorical_cols)
all_cols = numerical_cols + list(encoded_categories)

coefficients = model_lr.best_estimator_.named_steps['classifier'].coef_[0]
feature_importance_lr = pd.DataFrame({'Feature': all_cols, 'Importance': coefficients})
top_10_features_lr = feature_importance_lr.sort_values(by='Importance', ascending=False).head(10)

# Plot
plt.figure(figsize=(10, 6))
plt.barh(top_10_features_lr['Feature'], top_10_features_lr['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Top 10 Feature Importance - Logistic Regression')
plt.gca().invert_yaxis()
plt.show()

"""# Model XGBOOST"""

xgb_model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', XGBClassifier())
])

# Menambahkan parameter yang akan diuji
param_space_xgb = {
    'classifier__n_estimators': [100, 500, 1000],
    'classifier__max_depth': [3, 5, 7],
}

model_xgb = RandomizedSearchCV(xgb_model, param_distributions=param_space_xgb, n_iter=10, cv=3, random_state=42)

model_xgb.fit(X_train, y_train)

# Tampilkan hasil XGBoost
print("Best Parameters (XGBoost):", model_xgb.best_params_)
print("Training Accuracy (XGBoost):", model_xgb.score(X_train, y_train))
print("Model Best Score (XGBoost):", model_xgb.best_score_)
print("Test Accuracy (XGBoost):", model_xgb.score(X_test, y_test))

"""# Evaluation XGBOOST"""

from sklearn.metrics import classification_report, confusion_matrix

# Prediksi label untuk data uji
y_pred = model_xgb.predict(X_test)

# Evaluasi performa model
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Menghitung probabilitas prediksi untuk kelas positif
y_pred_proba = model_xgb.predict_proba(X_test)[:, 1]

# Menghitung nilai ROC AUC
roc_auc = roc_auc_score(y_test, y_pred_proba)
print("ROC AUC Score:", roc_auc)

# Menghitung kurva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Plotting kurva ROC
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""# Feature Importance Model XGBOOST"""

from sklearn.preprocessing import OneHotEncoder

# Define categorical columns
categorical_cols = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',
                    'NAME_FAMILY_STATUS', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE']

# Preprocessor for categorical features
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine transformers
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Create XGBoost pipeline
xgb_model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', XGBClassifier(n_estimators=100, max_depth=3))
])

xgb_model.fit(X_train, y_train)


feature_importance = xgb_model.named_steps['classifier'].feature_importances_
encoded_categorical_cols = xgb_model.named_steps['preprocessor'].transformers_[1][1]\
                                .named_steps['onehot'].get_feature_names_out(input_features=categorical_cols)
feature_names = np.concatenate([numerical_cols, encoded_categorical_cols])

feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})
top_10_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(10)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(top_10_features['Feature'], top_10_features['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Top 10 Important Features (XGBoost)')
plt.gca().invert_yaxis()
plt.show()

